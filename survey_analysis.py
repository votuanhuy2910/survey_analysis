# -*- coding: utf-8 -*-
"""survey_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pl8VNqSTggUbV6zgJABO8436t0kKq3Eb

# RandomForest
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np  # Import thư viện NumPy

# Đọc dữ liệu từ file CSV
data = pd.read_csv('/content/data_HaiLong_monthi_chuan_2 - Copy.csv', delimiter=';')

# Hiển thị phân bố của biến mục tiêu để kiểm tra mức độ mất cân bằng
print("Phân bố của các lớp trước khi huấn luyện:")
print(data['target_tag'].value_counts())

# Chia dữ liệu thành features (biến độc lập) và target (biến phụ thuộc)
X = data.drop('target_tag', axis=1)  # Loại bỏ cột mục tiêu khỏi dữ liệu để tạo features
y = data['target_tag']  # Cột mục tiêu

# Chia dữ liệu thành tập huấn luyện và tập kiểm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)

# Khởi tạo và huấn luyện mô hình Random Forest
random_forest = RandomForestClassifier(n_estimators=100, random_state=42)
random_forest.fit(X_train, y_train)

# Đánh giá mô hình trên tập kiểm tra
y_pred = random_forest.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

# In kết quả đánh giá
print("Độ chính xác của mô hình:", accuracy)
print("Báo cáo phân loại:\n", classification_rep)

# Tạo ma trận nhầm lẫn
conf_matrix = confusion_matrix(y_test, y_pred)

# Trực quan hóa ma trận nhầm lẫn bằng biểu đồ nhiệt (Heatmap)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Trực quan hóa tầm quan trọng của các đặc trưng
importances = random_forest.feature_importances_
indices = np.argsort(importances)[::-1]
feature_names = X.columns

plt.figure(figsize=(12, 8))
plt.title('Feature Importances')
plt.bar(range(X.shape[1]), importances[indices], align='center')
plt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=90)
plt.xlabel('Features')
plt.ylabel('Importance')
plt.show()

"""# randomforest_canbandulieu_RandomOverSampler"""

# Import các thư viện cần thiết
import pandas as pd
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler  # Import RandomOverSampler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Đọc dữ liệu
data = pd.read_csv('/content/data_HaiLong_monthi.csv', delimiter=';')

# Hiển thị phân bố của biến mục tiêu
print("Phân bố ban đầu của các lớp:", data['target_tag'].value_counts())

# Chia dữ liệu thành features và target
X = data.drop('target_tag', axis=1)  # Đảm bảo tên cột đúng
y = data['target_tag']

# Chia dữ liệu thành tập huấn luyện và kiểm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Cài đặt Random Over-Sampling
ros = RandomOverSampler(random_state=42)
X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)

# Hiển thị phân bố mới của các lớp sau khi áp dụng over-sampling
print("Phân bố sau khi áp dụng Over-Sampling:", pd.Series(y_train_resampled).value_counts())

# Huấn luyện mô hình Random Forest
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train_resampled, y_train_resampled)

# Đánh giá mô hình
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("Độ chính xác:", accuracy)
print("Báo cáo phân loại:\n", report)

"""# SVM"""

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# Đọc dữ liệu từ file CSV
file_path = '/content/data_HaiLong_monthi_chuan_2 - Copy.csv'
data = pd.read_csv(file_path, delimiter=';')

# Chuẩn bị dữ liệu
columns_to_plot = [
    'DoHaiLong', 'MonThiTruoc', 'Noidungcauhoiduocdiendatdehieurorang',
    'Noidungcauhoinamtrongchuongtrinhhoc', 'Noidungcauhoihaysangtao',
    'Caccongthuchienthirorangdedoc', 'Cauhoikhongcocacloivechinhtadiendat',
    'Cauhoicomucdokhophuhopvoinangluccuabanthan',
    'Tilecauhoitracnghiemvatraloingantrongdethilaphuhop', 'Thoigianlambaihoply'
]

# Chuyển đổi nhãn môn học thành số
#data['target_tag'] = data['target_tag'].map({'Toanhoc': 0, 'Vatli': 1, 'Hoahoc': 2,'Sinhhoc': 3,'Nguvan': 4,'Tienganh': 5})

# Tạo thêm đặc trưng
data['total_score'] = data[columns_to_plot].sum(axis=1)

# Chia dữ liệu thành các biến độc lập X và biến phụ thuộc y
X = data[columns_to_plot + ['total_score']]
y = data['target_tag']

# Chia dữ liệu thành tập huấn luyện và tập kiểm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Cân bằng dữ liệu bằng SMOTE
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

# Chuẩn hóa dữ liệu
scaler = StandardScaler()
X_train_balanced = scaler.fit_transform(X_train_balanced)
X_test = scaler.transform(X_test)

# Sử dụng GridSearchCV để tìm các siêu tham số tốt nhất cho mô hình SVM
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': [1, 0.1, 0.01],
    'kernel': ['rbf', 'poly', 'sigmoid']
}
grid_search = GridSearchCV(estimator=SVC(random_state=42), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(X_train_balanced, y_train_balanced)

# Huấn luyện mô hình với các siêu tham số tốt nhất
best_model = grid_search.best_estimator_
best_model.fit(X_train_balanced, y_train_balanced)

# Dự đoán trên tập kiểm tra
y_pred = best_model.predict(X_test)

# Đánh giá mô hình
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Classification Report:\n{report}')

# Trực quan hóa kết quả
plt.figure(figsize=(10, 6))
sns.heatmap(pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).iloc[:-1, :].T, annot=True)
plt.title('Classification Report Heatmap')
plt.show()

"""# EDA"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
data_path = '/content/data_HaiLong_monthi_chuan_2.csv'
data = pd.read_csv(data_path, sep=';')

# 1. Kiểm tra giá trị thiếu
missing_values = data.isnull().sum()
print("Missing Values:\n", missing_values)

# 2. Kiểm tra giá trị ngoại lệ
data_numeric = data.select_dtypes(include=[np.number])
for column in data_numeric.columns:
    plt.figure(figsize=(10, 5))
    plt.boxplot(data_numeric[column], vert=False)
    plt.title(f'Box plot of {column}')
    plt.show()

# 3. Kiểm tra sự phân bố
for column in data_numeric.columns:
    plt.figure(figsize=(10, 5))
    sns.histplot(data_numeric[column], kde=True)
    plt.title(f'Distribution of {column}')
    plt.show()

# 4. Kiểm tra giá trị trùng lặp
duplicates = data.duplicated().sum()
print("Duplicate Rows:", duplicates)

# 5. Kiểm tra tương quan
correlation_matrix = data_numeric.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# 6. Phân tích dữ liệu phân loại
data_categorical = data.select_dtypes(include=['object'])
for column in data_categorical.columns:
    plt.figure(figsize=(10, 5))
    sns.countplot(y=column, data=data_categorical, order=data_categorical[column].value_counts().index)
    plt.title(f'Distribution of {column}')
    plt.show()

# 7. Kiểm tra phân bố nhãn mục tiêu
y = data['target_tag']
plt.figure(figsize=(10, 5))
sns.countplot(y=y, order=y.value_counts().index)
plt.title('Distribution of Target')
plt.show()
import seaborn as sns
import matplotlib.pyplot as plt

# Tạo các bảng màu khác nhau
palette = sns.color_palette("Set1", n_colors=len(data['target_tag'].unique()))

# Tính toán điểm trung bình cho từng tiêu chí theo môn học
mean_scores = data.groupby('target_tag').mean().reset_index()

# Tạo biểu đồ thanh cho từng tiêu chí đánh giá theo môn học
mean_scores_melted = pd.melt(mean_scores, id_vars=['target_tag'], value_vars=columns_to_plot)

plt.figure(figsize=(15, 10))
sns.barplot(x='variable', y='value', hue='target_tag', data=mean_scores_melted, palette=palette)
plt.xticks(rotation=90)
plt.title('Average Scores for Each Evaluation Criterion by Subject')
plt.xlabel('Evaluation Criteria')
plt.ylabel('Average Scores')
plt.legend(title='Subject')
plt.show()

columns_to_plot = [
    'DoHaiLong', 'Noidungcauhoiduocdiendatdehieurorang',
    'Noidungcauhoinamtrongchuongtrinhhoc', 'Noidungcauhoihaysangtao',
    'Caccongthuchienthirorangdedoc', 'Cauhoikhongcocacloivechinhtadiendat',
    'Cauhoicomucdokhophuhopvoinangluccuabanthan',
    'Tilecauhoitracnghiemvatraloingantrongdethilaphuhop', 'Thoigianlambaihoply'
]

# Tính toán các giá trị trung bình, trung vị và số mode cho các cột đã chọn
mean_values = data[columns_to_plot].mean()
median_values = data[columns_to_plot].median()
mode_values = data[columns_to_plot].mode().iloc[0]

# Tạo DataFrame để hiển thị các giá trị này
summary_stats = pd.DataFrame({
    'Mean' mean_values,
    'Median' median_values,
    'Mode' mode_values
})

# Hiển thị DataFrame
print(summary_stats)

# Vẽ biểu đồ các giá trị trung bình
plt.figure(figsize=(10, 6))
summary_stats['Mean'].plot(kind='line', title='Mean Values')
plt.xlabel('Criteria')
plt.ylabel('Mean Value')
plt.xticks(rotation=45, ha='right')
plt.gca().spines[['top', 'right']].set_visible(False)
plt.tight_layout()
plt.show()

columns_to_plot = [
    'DoHaiLong', 'Noidungcauhoiduocdiendatdehieurorang',
    'Noidungcauhoinamtrongchuongtrinhhoc', 'Noidungcauhoihaysangtao',
    'Caccongthuchienthirorangdedoc', 'Cauhoikhongcocacloivechinhtadiendat',
    'Cauhoicomucdokhophuhopvoinangluccuabanthan',
    'Tilecauhoitracnghiemvatraloingantrongdethilaphuhop', 'Thoigianlambaihoply'
]

# Tính toán các giá trị trung bình, trung vị và số mode cho các cột đã chọn
mean_values = data[columns_to_plot].mean()
median_values = data[columns_to_plot].median()
mode_values = data[columns_to_plot].mode().iloc[0]

# Tạo DataFrame để hiển thị các giá trị này
summary_stats = pd.DataFrame({
    'Mean' mean_values,
    'Median' median_values,
    'Mode' mode_values
})

# Hiển thị DataFrame
print(summary_stats)

# Vẽ biểu đồ các giá trị trung bình
plt.figure(figsize=(10, 6))
summary_stats['Mean'].plot(kind='line', title='Mean Values')
plt.xlabel('Criteria')
plt.ylabel('Mean Value')
plt.xticks(rotation=45, ha='right')
plt.gca().spines[['top', 'right']].set_visible(False)
plt.tight_layout()
plt.show()

"""# toiUuRandomForest_XGBoost"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
data_path = '/content/data_HaiLong_monthi_chuan_2.csv'
data = pd.read_csv(data_path, sep=';')

# Display the first few rows of the dataset
print(data.head())

# Define the features and the target
X = data.drop(columns=['target_tag'])
y = data['target_tag']

# Splitting the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Encode the target variable
encoder = LabelEncoder()
y_train_encoded = encoder.fit_transform(y_train)
y_test_encoded = encoder.transform(y_test)

# Initialize the XGBClassifier
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')

# Define the parameter grid for XGBoost
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [4, 6, 8],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9]
}

# Setup the GridSearchCV for XGBoost
grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, cv=5, n_jobs=-1, verbose=2)

# Train the model with Grid Search and cross-validation for XGBoost
grid_search_xgb.fit(X_train, y_train_encoded)

# Get the best parameters for XGBoost
best_params_xgb = grid_search_xgb.best_params_
print("Best parameters for XGBoost found: ", best_params_xgb)

# Train the final XGBoost model with the best parameters
final_xgb_model = XGBClassifier(**best_params_xgb, use_label_encoder=False, eval_metric='mlogloss')
final_xgb_model.fit(X_train, y_train_encoded)

# Predictions with XGBoost
y_pred_xgb = final_xgb_model.predict(X_test)

# Evaluate the XGBoost model
accuracy_xgb = accuracy_score(y_test_encoded, y_pred_xgb)
report_xgb = classification_report(y_test_encoded, y_pred_xgb)

print("XGBoost Accuracy:", accuracy_xgb)
print("XGBoost Classification Report:\n", report_xgb)

# Initialize the RandomForestClassifier
rf_model = RandomForestClassifier()

# Define the parameter grid for RandomForest
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [4, 6, 8],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Setup the GridSearchCV for RandomForest
grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, cv=5, n_jobs=-1, verbose=2)

# Train the model with Grid Search and cross-validation for RandomForest
grid_search_rf.fit(X_train, y_train_encoded)

# Get the best parameters for RandomForest
best_params_rf = grid_search_rf.best_params_
print("Best parameters for RandomForest found: ", best_params_rf)

# Train the final RandomForest model with the best parameters
final_rf_model = RandomForestClassifier(**best_params_rf)
final_rf_model.fit(X_train, y_train_encoded)

# Predictions with RandomForest
y_pred_rf = final_rf_model.predict(X_test)

# Evaluate the RandomForest model
accuracy_rf = accuracy_score(y_test_encoded, y_pred_rf)
report_rf = classification_report(y_test_encoded, y_pred_rf)

print("RandomForest Accuracy:", accuracy_rf)
print("RandomForest Classification Report:\n", report_rf)

# Compare the results
print("XGBoost Accuracy:", accuracy_xgb)
print("RandomForest Accuracy:", accuracy_rf)

"""# toiuuXGBoost"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE

# Load the data
data_path = '/content/data_HaiLong_monthi_chuan_2.csv'  # Update this path to your file location
data = pd.read_csv(data_path, sep=';')

# Display the first few rows of the dataset
print(data.head())

# Define the features and the target
X = data.drop(columns=['target_tag'])
y = data['target_tag']

# Encode the target variable
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)

# Splitting the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Balance the training data using SMOTE
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

# Initialize the XGBClassifier
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [4, 6, 8],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9]
}

# Setup the GridSearchCV with cross-validation
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)

# Train the model with Grid Search and cross-validation
grid_search.fit(X_train_balanced, y_train_balanced)

# Get the best parameters
best_params = grid_search.best_params_
print("Best parameters found: ", best_params)

# Train the final model with the best parameters
final_xgb_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='mlogloss')
final_xgb_model.fit(X_train_balanced, y_train_balanced)

# Predictions
y_pred = final_xgb_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.classes_, yticklabels=encoder.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Plot feature importance
plt.figure(figsize=(12, 8))
xgb.plot_importance(final_xgb_model, max_num_features=10)
plt.title('Feature Importance')
plt.show()

"""# tonghopthuattoan"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load the data
data_path = '/content/data_HaiLong_monthi_chuan_2 - Copy.csv'
data = pd.read_csv(data_path, sep=';')

# Define the features and the target
X = data.drop(columns=['target_tag'])
y = data['target_tag']

# Splitting the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Encode the target variable
encoder = LabelEncoder()
y_train_encoded = encoder.fit_transform(y_train)
y_test_encoded = encoder.transform(y_test)

# Initialize the classifiers
classifiers = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),
    'XGBoost': XGBClassifier(n_estimators=11, learning_rate=0.1, max_depth=3, random_state=11),
    'K-NN': KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k) as needed
}

# Train and evaluate each classifier
results = {}

for name, clf in classifiers.items():
    clf.fit(X_train, y_train_encoded)
    y_pred = clf.predict(X_test)

    accuracy = accuracy_score(y_test_encoded, y_pred)
    report = classification_report(y_test_encoded, y_pred, output_dict=True)
    conf_matrix = confusion_matrix(y_test_encoded, y_pred)

    results[name] = {
        'accuracy': accuracy,
        'report': report,
        'conf_matrix': conf_matrix
    }

    # Print the results
    print(f"Classifier: {name}")
    print(f"Accuracy: {accuracy}")
    print(f"Classification Report:\n{classification_report(y_test_encoded, y_pred)}")

    # Plot confusion matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.classes_, yticklabels=encoder.classes_)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(f'Confusion Matrix for {name}')
    plt.show()

# Summarize results
for name, result in results.items():
    print(f"\n{name} Model Analysis:")
    print(f"Accuracy: {result['accuracy']}")
    print("Classification Report:")
    print(pd.DataFrame(result['report']).transpose())

"""# XGBoost"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
data_path = '/content/data_HaiLong_monthi_chuan_2.csv'  # Update this path to your file location
data = pd.read_csv(data_path, sep=';')

# Display the first few rows of the dataset
print(data.head())

# Define the features and the target
X = data.drop(columns=['target_tag'])
y = data['target_tag']

# Splitting the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Encode the target variable
encoder = LabelEncoder()
y_train_encoded = encoder.fit_transform(y_train)
y_test_encoded = encoder.transform(y_test)

# Initialize the XGBClassifier
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [4, 6, 8],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9]
}

# Setup the GridSearchCV with cross-validation
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)

# Train the model with Grid Search and cross-validation
grid_search.fit(X_train, y_train_encoded)

# Get the best parameters
best_params = grid_search.best_params_
print("Best parameters found: ", best_params)

# Train the final model with the best parameters
final_xgb_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='mlogloss')
final_xgb_model.fit(X_train, y_train_encoded)

# Predictions
y_pred = final_xgb_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test_encoded, y_pred)
report = classification_report(y_test_encoded, y_pred)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)

# Confusion matrix
conf_matrix = confusion_matrix(y_test_encoded, y_pred)

# Plot confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.classes_, yticklabels=encoder.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Plot feature importance
plt.figure(figsize=(12, 8))
xgb.plot_importance(final_xgb_model, max_num_features=10)
plt.title('Feature Importance')
plt.show()

# Perform cross-validation
cv_scores = cross_val_score(final_xgb_model, X, y, cv=5)
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation score:", cv_scores.mean())